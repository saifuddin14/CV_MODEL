{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saifuddin14/CV_MODEL/blob/main/(NLP)_GB_M8L1_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing for NLP**\n",
        "In this lab we will learn how to preprocess raw text data and make it suitable for further processing by DL models. We will perform this processing in two ways.\n",
        "1. Using NLTK Library\n",
        "2. Using Tensorflow\n",
        "First, let's see how we can preprocess raw text data using NLTK.\n",
        "\n",
        "Import NLTK library and download relevant modules."
      ],
      "metadata": {
        "id": "Qa8x0dIl2JES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwqbPqbOn0dO",
        "outputId": "b39269d5-f081-4f37-b839-0040c5e7dd33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "random.seed(92)\n",
        "#Note that you will need to import the nltk library and download its resources before running the code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a corpus of raw text. For this example the following text is taken from wikipedia page on NLP (https://en.wikipedia.org/wiki/Natural_language_processing).\n",
        "You will notice that the text contains words, numbers, punctuations, and other special characters. This is RAW form of text. For different NLP tasks, it is required to preprocess this data to make it more structures and workable."
      ],
      "metadata": {
        "id": "0VBcykaE27VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = str([\"\"\"In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[8]\n",
        "In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[9] and in the following years he went on to develop Word2vec.\n",
        "In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing.\n",
        "That popularity was due partly to a flurry of results showing that such techniques[10][11] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[12] and parsing.[13][14]\n",
        "This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[15] or protect patient privacy.[16]\"\"\"])"
      ],
      "metadata": {
        "id": "q1Hk9thOoo9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "vaRhM8GKPo4m",
        "outputId": "95ab29f4-e1b4-4cd3-ffe6-5b2606f7921e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[8]\\\\nIn 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[9] and in the following years he went on to develop Word2vec.\\\\nIn the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing.\\\\nThat popularity was due partly to a flurry of results showing that such techniques[10][11] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[12] and parsing.[13][14]\\\\nThis is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[15] or protect patient privacy.[16]']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "Tokenization means dividing a piece of raw text into semantically and syntactically meaningful units, called tokens. These tokens can be at setence-level (sentence tokens) or at the word-level (word tokens). At word level, NLTK recognises each word seperated by white-space, number, and punctuations as distinct tokens.\n",
        "\n",
        "The following code passes the raw text (corpus) to the sent_tokenize() method which converts the text into sentences. Each sentence is then further tokenized into smaller units using word_tokenize() method.\n",
        "\n",
        "pos_tag() method and ne_chunk() methods perform Part-Of-Speech tagging and Named-Entity Recognition, respectively. The POS tagging is a task in which each word is tagged with a part-of-speech category. For example, the word 'horse' is tagged as a noun and 'walk' is tagged as a verb. A Named-Entity is a word (or a set of words) that represent name of a person, place, organisation, unit of measurement, etc."
      ],
      "metadata": {
        "id": "Nsqvjw614EkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "token_list = []\n",
        "sentences = nltk.sent_tokenize(str(corpus))\n",
        "for sentence in sentences:\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  token_list.extend(words)  #append() adds a single element to the end of the list while, extend() can add multiple individual elements to the end of the list\n",
        "  tagged_words = nltk.pos_tag(words)\n",
        "  named_entities = nltk.ne_chunk(tagged_words)\n",
        "\n",
        "print(token_list)\n",
        "print ('\\nLength of word_list:', len(token_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUj2pksvo4ul",
        "outputId": "3aa91eca-3667-4a26-ac4c-18bcb7461f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', \"'In\", '2003', ',', 'word', 'n-gram', 'model', ',', 'at', 'the', 'time', 'the', 'best', 'statistical', 'algorithm', ',', 'was', 'overperformed', 'by', 'a', 'multi-layer', 'perceptron', '(', 'with', 'a', 'single', 'hidden', 'layer', 'and', 'context', 'length', 'of', 'several', 'words', 'trained', 'on', 'up', 'to', '14', 'million', 'of', 'words', 'with', 'a', 'CPU', 'cluster', 'in', 'language', 'modelling', ')', 'by', 'Yoshua', 'Bengio', 'with', 'co-authors', '.', '[', '8', ']', '\\\\nIn', '2010', ',', 'Tomáš', 'Mikolov', '(', 'then', 'a', 'PhD', 'student', 'at', 'Brno', 'University', 'of', 'Technology', ')', 'with', 'co-authors', 'applied', 'a', 'simple', 'recurrent', 'neural', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'to', 'language', 'modelling', ',', '[', '9', ']', 'and', 'in', 'the', 'following', 'years', 'he', 'went', 'on', 'to', 'develop', 'Word2vec.\\\\nIn', 'the', '2010s', ',', 'representation', 'learning', 'and', 'deep', 'neural', 'network-style', '(', 'featuring', 'many', 'hidden', 'layers', ')', 'machine', 'learning', 'methods', 'became', 'widespread', 'in', 'natural', 'language', 'processing.\\\\nThat', 'popularity', 'was', 'due', 'partly', 'to', 'a', 'flurry', 'of', 'results', 'showing', 'that', 'such', 'techniques', '[', '10', ']', '[', '11', ']', 'can', 'achieve', 'state-of-the-art', 'results', 'in', 'many', 'natural', 'language', 'tasks', ',', 'e.g.', ',', 'in', 'language', 'modeling', '[', '12', ']', 'and', 'parsing', '.', '[', '13', ']', '[', '14', ']', '\\\\nThis', 'is', 'increasingly', 'important', 'in', 'medicine', 'and', 'healthcare', ',', 'where', 'NLP', 'helps', 'analyze', 'notes', 'and', 'text', 'in', 'electronic', 'health', 'records', 'that', 'would', 'otherwise', 'be', 'inaccessible', 'for', 'study', 'when', 'seeking', 'to', 'improve', 'care', '[', '15', ']', 'or', 'protect', 'patient', 'privacy', '.', '[', '16', ']', \"'\", ']']\n",
            "\n",
            "Length of word_list: 221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to tokenize a piece of text using NLTK is to use the split() method as shown below."
      ],
      "metadata": {
        "id": "hmVcmxNoCIQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = corpus.split()\n",
        "print(word_list)\n",
        "print ('\\nLength of word_list:', len(word_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzxM_dzpq2Iw",
        "outputId": "531a21cd-3e61-46b0-b9a2-7ecb5f2e8134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"['In\", '2003,', 'word', 'n-gram', 'model,', 'at', 'the', 'time', 'the', 'best', 'statistical', 'algorithm,', 'was', 'overperformed', 'by', 'a', 'multi-layer', 'perceptron', '(with', 'a', 'single', 'hidden', 'layer', 'and', 'context', 'length', 'of', 'several', 'words', 'trained', 'on', 'up', 'to', '14', 'million', 'of', 'words', 'with', 'a', 'CPU', 'cluster', 'in', 'language', 'modelling)', 'by', 'Yoshua', 'Bengio', 'with', 'co-authors.[8]\\\\nIn', '2010,', 'Tomáš', 'Mikolov', '(then', 'a', 'PhD', 'student', 'at', 'Brno', 'University', 'of', 'Technology)', 'with', 'co-authors', 'applied', 'a', 'simple', 'recurrent', 'neural', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'to', 'language', 'modelling,[9]', 'and', 'in', 'the', 'following', 'years', 'he', 'went', 'on', 'to', 'develop', 'Word2vec.\\\\nIn', 'the', '2010s,', 'representation', 'learning', 'and', 'deep', 'neural', 'network-style', '(featuring', 'many', 'hidden', 'layers)', 'machine', 'learning', 'methods', 'became', 'widespread', 'in', 'natural', 'language', 'processing.\\\\nThat', 'popularity', 'was', 'due', 'partly', 'to', 'a', 'flurry', 'of', 'results', 'showing', 'that', 'such', 'techniques[10][11]', 'can', 'achieve', 'state-of-the-art', 'results', 'in', 'many', 'natural', 'language', 'tasks,', 'e.g.,', 'in', 'language', 'modeling[12]', 'and', 'parsing.[13][14]\\\\nThis', 'is', 'increasingly', 'important', 'in', 'medicine', 'and', 'healthcare,', 'where', 'NLP', 'helps', 'analyze', 'notes', 'and', 'text', 'in', 'electronic', 'health', 'records', 'that', 'would', 'otherwise', 'be', 'inaccessible', 'for', 'study', 'when', 'seeking', 'to', 'improve', 'care[15]', 'or', 'protect', 'patient', \"privacy.[16]']\"]\n",
            "\n",
            "Length of word_list: 171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's the difference between tokens obtained through word_tokenize() method and through split() method?\n",
        "\n",
        "Next, we filter the tokens and keep only those that contains alphabets."
      ],
      "metadata": {
        "id": "UBslCXNHCTs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabets_only = [word for word in word_list if word.isalpha()]\n",
        "print(alphabets_only)\n",
        "print ('\\nLength of word_list:', len(alphabets_only))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qqLc00Zzd5I",
        "outputId": "4bcfbaf9-7882-490d-cc30-6e7946c8d1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['word', 'at', 'the', 'time', 'the', 'best', 'statistical', 'was', 'overperformed', 'by', 'a', 'perceptron', 'a', 'single', 'hidden', 'layer', 'and', 'context', 'length', 'of', 'several', 'words', 'trained', 'on', 'up', 'to', 'million', 'of', 'words', 'with', 'a', 'CPU', 'cluster', 'in', 'language', 'by', 'Yoshua', 'Bengio', 'with', 'Tomáš', 'Mikolov', 'a', 'PhD', 'student', 'at', 'Brno', 'University', 'of', 'with', 'applied', 'a', 'simple', 'recurrent', 'neural', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'to', 'language', 'and', 'in', 'the', 'following', 'years', 'he', 'went', 'on', 'to', 'develop', 'the', 'representation', 'learning', 'and', 'deep', 'neural', 'many', 'hidden', 'machine', 'learning', 'methods', 'became', 'widespread', 'in', 'natural', 'language', 'popularity', 'was', 'due', 'partly', 'to', 'a', 'flurry', 'of', 'results', 'showing', 'that', 'such', 'can', 'achieve', 'results', 'in', 'many', 'natural', 'language', 'in', 'language', 'and', 'is', 'increasingly', 'important', 'in', 'medicine', 'and', 'where', 'NLP', 'helps', 'analyze', 'notes', 'and', 'text', 'in', 'electronic', 'health', 'records', 'that', 'would', 'otherwise', 'be', 'inaccessible', 'for', 'study', 'when', 'seeking', 'to', 'improve', 'or', 'protect', 'patient']\n",
            "\n",
            "Length of word_list: 141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Normalisation** means to remove any capitalisation in the text and convert all tokens to lower case."
      ],
      "metadata": {
        "id": "CR29nqjbDUj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_case = [word.lower() for word in alphabets_only]\n",
        "print(lower_case)\n",
        "print ('\\nLength of word_list:', len(lower_case))"
      ],
      "metadata": {
        "id": "gBXX3g7l0LyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword removal means to remove those words from the remaining tokens that appear frequently in English language but contribute very little to the overall meaning of the text. It is important to note here that stopword removal should not be applied to every NLP task since in some application these stopwords are important as well."
      ],
      "metadata": {
        "id": "p5pYZcPFDrKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_nltk = set(stopwords.words('english'))\n",
        "print(stopwords_nltk)\n",
        "print ('\\nLength of word_list:', len(stopwords_nltk), '\\n')\n",
        "\n",
        "cleaned_words = [word for word in lower_case if word not in stopwords_nltk]\n",
        "print(cleaned_words)\n",
        "print ('\\nLength of word_list:', len(cleaned_words))"
      ],
      "metadata": {
        "id": "XW0Qe6nL0iQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_words = stopwords.words('english')  # Get the list of English stopwords\n",
        "# print(stop_words)\n",
        "# available_languages = stopwords.fileids()\n",
        "# print(available_languages)"
      ],
      "metadata": {
        "id": "xioas09zcvVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This concludes the first part of this lab which deals with preprocessing raw text using NLTK. Now Let's use tensorfolow to perform similar preprocessing (note the difference in the implementation of both libraries) and build a complete example of sentiment analysis."
      ],
      "metadata": {
        "id": "IfQtkxeIENaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "id": "6mQ25HF5r6ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")   #<OOV> means \"Out of Vocabulary\"\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=5)\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)"
      ],
      "metadata": {
        "id": "EglaZfzNwtns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ],
      "metadata": {
        "id": "1PTErLmdvBFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json"
      ],
      "metadata": {
        "id": "VtEnSsGhw7Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])"
      ],
      "metadata": {
        "id": "It80kX2I0Zin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "metadata": {
        "id": "hPqjSK960d8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "metadata": {
        "id": "jUtxvaJH0hec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need this block to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "JuXb3H750jXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JIFR8YFm0ng7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
      ],
      "metadata": {
        "id": "wEzL76dc0q06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "--qpISlW0t-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))"
      ],
      "metadata": {
        "id": "FECUG7ZC0zFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\"i quick brown fox jump over the lazy dog\", \"circuit is the path through which current can flow\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))"
      ],
      "metadata": {
        "id": "QZEEtqboiQJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OjfCnLq1jIIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}